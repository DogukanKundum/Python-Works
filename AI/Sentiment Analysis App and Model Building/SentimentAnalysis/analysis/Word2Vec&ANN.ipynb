{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec,Word2Vec\n",
    "from gensim.utils import tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alway wrote seri complet stinkfest jim belushi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st watch 10dirstev purcel typic mari kate ash...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movi poorli written direct fell asleep minut m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interest thing miryang secret sunshin actor je...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>first read berlin meer didnt expect much thoug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  alway wrote seri complet stinkfest jim belushi...      0\n",
       "1  1st watch 10dirstev purcel typic mari kate ash...      0\n",
       "2  movi poorli written direct fell asleep minut m...      0\n",
       "3  interest thing miryang secret sunshin actor je...      1\n",
       "4  first read berlin meer didnt expect much thoug...      0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    24884\n",
       "0    24698\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_label(review):\n",
    "    review_text = review\n",
    "    words = review_text.split()\n",
    "    return (words)\n",
    "def labels_to_sentences(label):\n",
    "    raw_sentences = sent_tokenize(label.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(preprocess_label(raw_sentence))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['alway',\n",
       "  'wrote',\n",
       "  'seri',\n",
       "  'complet',\n",
       "  'stinkfest',\n",
       "  'jim',\n",
       "  'belushi',\n",
       "  'involv',\n",
       "  'heavili',\n",
       "  'one',\n",
       "  'day',\n",
       "  'tragic',\n",
       "  'happenst',\n",
       "  'occur',\n",
       "  'white',\n",
       "  'sox',\n",
       "  'game',\n",
       "  'end',\n",
       "  'realiz',\n",
       "  'remot',\n",
       "  'way',\n",
       "  'side',\n",
       "  'room',\n",
       "  'somehow',\n",
       "  'could',\n",
       "  'gotten',\n",
       "  'walk',\n",
       "  'across',\n",
       "  'room',\n",
       "  'get',\n",
       "  'remot',\n",
       "  'even',\n",
       "  'tv',\n",
       "  'turn',\n",
       "  'channel',\n",
       "  'get',\n",
       "  'walk',\n",
       "  'across',\n",
       "  'countri',\n",
       "  'watch',\n",
       "  'tv',\n",
       "  'anoth',\n",
       "  'state',\n",
       "  'nut',\n",
       "  'said',\n",
       "  'decid',\n",
       "  'hang',\n",
       "  'tight',\n",
       "  'couch',\n",
       "  'take',\n",
       "  'whatev',\n",
       "  'fate',\n",
       "  'store',\n",
       "  'fate',\n",
       "  'store',\n",
       "  'episod',\n",
       "  'show',\n",
       "  'episod',\n",
       "  'rememb',\n",
       "  'littl',\n",
       "  'except',\n",
       "  'made',\n",
       "  'broad',\n",
       "  'gener',\n",
       "  'sweep',\n",
       "  'blanket',\n",
       "  'judgment',\n",
       "  'base',\n",
       "  'zero',\n",
       "  'object',\n",
       "  'experienti',\n",
       "  'evid',\n",
       "  'noth',\n",
       "  'whatsoev',\n",
       "  'back',\n",
       "  'opinion',\n",
       "  'complet',\n",
       "  'right',\n",
       "  'show',\n",
       "  'total',\n",
       "  'crudpi',\n",
       "  'belushi',\n",
       "  'comed',\n",
       "  'deliveri',\n",
       "  'hairi',\n",
       "  'lighthous',\n",
       "  'foghorn',\n",
       "  'women',\n",
       "  'physic',\n",
       "  'attract',\n",
       "  'stepfordi',\n",
       "  'elicit',\n",
       "  'real',\n",
       "  'feel',\n",
       "  'viewer',\n",
       "  'absolut',\n",
       "  'reason',\n",
       "  'stop',\n",
       "  'run',\n",
       "  'local',\n",
       "  'tv',\n",
       "  'station',\n",
       "  'gasolin',\n",
       "  'flamethrow',\n",
       "  'send',\n",
       "  'everi',\n",
       "  'copi',\n",
       "  'mutt',\n",
       "  'howl',\n",
       "  'back',\n",
       "  'hell',\n",
       "  'br',\n",
       "  'br',\n",
       "  'except',\n",
       "  'br',\n",
       "  'br',\n",
       "  'except',\n",
       "  'wonder',\n",
       "  'comic',\n",
       "  'sti',\n",
       "  'ling',\n",
       "  'larri',\n",
       "  'joe',\n",
       "  'campbel',\n",
       "  'america',\n",
       "  'greatest',\n",
       "  'comic',\n",
       "  'charact',\n",
       "  'actor',\n",
       "  'guy',\n",
       "  'play',\n",
       "  'belushi',\n",
       "  'brotherinlaw',\n",
       "  'andi',\n",
       "  'gold',\n",
       "  'good',\n",
       "  'realli',\n",
       "  'well',\n",
       "  'asid',\n",
       "  'funni',\n",
       "  'job',\n",
       "  'make',\n",
       "  'belushi',\n",
       "  'look',\n",
       "  'good',\n",
       "  'that',\n",
       "  'like',\n",
       "  'tri',\n",
       "  'make',\n",
       "  'butt',\n",
       "  'wart',\n",
       "  'look',\n",
       "  'good',\n",
       "  'campbel',\n",
       "  'pull',\n",
       "  'style',\n",
       "  'someon',\n",
       "  'invent',\n",
       "  'nobel',\n",
       "  'prize',\n",
       "  'comic',\n",
       "  'buffooneri',\n",
       "  'win',\n",
       "  'everi',\n",
       "  'year',\n",
       "  'without',\n",
       "  'larri',\n",
       "  'joe',\n",
       "  'show',\n",
       "  'would',\n",
       "  'consist',\n",
       "  'slightli',\n",
       "  'vacant',\n",
       "  'look',\n",
       "  'courtney',\n",
       "  'thornesmith',\n",
       "  'smack',\n",
       "  'belushi',\n",
       "  'head',\n",
       "  'fri',\n",
       "  'pan',\n",
       "  'altern',\n",
       "  'beat',\n",
       "  'chest',\n",
       "  'play',\n",
       "  'straw',\n",
       "  'floor',\n",
       "  'cage',\n",
       "  'star',\n",
       "  'larri',\n",
       "  'joe',\n",
       "  'campbel',\n",
       "  'design',\n",
       "  'comed',\n",
       "  'bacon',\n",
       "  'improv',\n",
       "  'flavor',\n",
       "  'everyth',\n",
       "  'he']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_sentences(data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49582"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "for review in data[\"text\"]:\n",
    "    if type(review) == float:\n",
    "        continue\n",
    "    sentences += labels_to_sentences(review)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent = [row.split() for row in data['text'] if not type(row) == float]\n",
    "#phrases = Phrases(sent, min_count=1, progress_per=10000)\n",
    "#sentences = phrases[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300  \n",
    "min_word_count = 3\n",
    "num_workers = 4     \n",
    "context = 10       \n",
    "downsampling = 1e-3 # (0.001) \n",
    "\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences,\\\n",
    "                          workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling,\n",
    "                          sg = 1\n",
    "                         )\n",
    "\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42553, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('greatit', 0.8054613471031189),\n",
       " ('nameand', 0.7951759099960327),\n",
       " ('ripa', 0.7915255427360535),\n",
       " ('thoe', 0.7837172746658325),\n",
       " ('appereantli', 0.7826181054115295),\n",
       " ('petsbr', 0.7802642583847046),\n",
       " ('yah', 0.7759445905685425),\n",
       " ('efron', 0.7745994329452515),\n",
       " ('funnyand', 0.7739788293838501),\n",
       " ('performedbr', 0.7736026644706726)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"awsom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/cloudcompt/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "def featureVecMethod(words, model, num_features):\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs\n",
    "data_vects = []\n",
    "for review in data['text']:\n",
    "        data_vects.append(preprocess_label(review))\n",
    "    \n",
    "dataAvgVects = getAvgFeatureVecs(data_vects, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataVects = []\n",
    "for item in dataAvgVects:\n",
    "    item[np.isnan(item)] = 0\n",
    "    dataVects.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49582"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataVects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49582"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataVects = np.asarray(dataVects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "y = encoder.fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataVects,y,test_size=0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9917, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=128,activation='relu',input_shape=(300,)))\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "\n",
    "classifier.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "classifier.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39665 samples, validate on 9917 samples\n",
      "Epoch 1/30\n",
      "39665/39665 [==============================] - 13s 330us/sample - loss: 0.3501 - accuracy: 0.8464 - val_loss: 0.3151 - val_accuracy: 0.8553\n",
      "Epoch 2/30\n",
      "39665/39665 [==============================] - 11s 277us/sample - loss: 0.3077 - accuracy: 0.8740 - val_loss: 0.2995 - val_accuracy: 0.8833\n",
      "Epoch 3/30\n",
      "39665/39665 [==============================] - 11s 284us/sample - loss: 0.2928 - accuracy: 0.8804 - val_loss: 0.2844 - val_accuracy: 0.8733\n",
      "Epoch 4/30\n",
      "39665/39665 [==============================] - 12s 290us/sample - loss: 0.2905 - accuracy: 0.8828 - val_loss: 0.2792 - val_accuracy: 0.8835\n",
      "Epoch 5/30\n",
      "39665/39665 [==============================] - 12s 292us/sample - loss: 0.2894 - accuracy: 0.8828 - val_loss: 0.2912 - val_accuracy: 0.8765\n",
      "Epoch 6/30\n",
      "39665/39665 [==============================] - 12s 291us/sample - loss: 0.2858 - accuracy: 0.8826 - val_loss: 0.2780 - val_accuracy: 0.8822\n",
      "Epoch 7/30\n",
      "39665/39665 [==============================] - 12s 291us/sample - loss: 0.2828 - accuracy: 0.8857 - val_loss: 0.2740 - val_accuracy: 0.8861\n",
      "Epoch 8/30\n",
      "39665/39665 [==============================] - 12s 293us/sample - loss: 0.2817 - accuracy: 0.8855 - val_loss: 0.2804 - val_accuracy: 0.8842\n",
      "Epoch 9/30\n",
      "39665/39665 [==============================] - 12s 291us/sample - loss: 0.2780 - accuracy: 0.8865 - val_loss: 0.2882 - val_accuracy: 0.8816\n",
      "Epoch 10/30\n",
      "39665/39665 [==============================] - 11s 290us/sample - loss: 0.2752 - accuracy: 0.8887 - val_loss: 0.2756 - val_accuracy: 0.8865\n",
      "Epoch 11/30\n",
      "39665/39665 [==============================] - 12s 290us/sample - loss: 0.2732 - accuracy: 0.8887 - val_loss: 0.2695 - val_accuracy: 0.8883\n",
      "Epoch 12/30\n",
      "39665/39665 [==============================] - 12s 292us/sample - loss: 0.2735 - accuracy: 0.8887 - val_loss: 0.2990 - val_accuracy: 0.8795\n",
      "Epoch 13/30\n",
      "39665/39665 [==============================] - 14s 355us/sample - loss: 0.2719 - accuracy: 0.8900 - val_loss: 0.2710 - val_accuracy: 0.8879\n",
      "Epoch 14/30\n",
      "39665/39665 [==============================] - 12s 297us/sample - loss: 0.2690 - accuracy: 0.8914 - val_loss: 0.2775 - val_accuracy: 0.8865\n",
      "Epoch 15/30\n",
      "39665/39665 [==============================] - 13s 326us/sample - loss: 0.2672 - accuracy: 0.8913 - val_loss: 0.2804 - val_accuracy: 0.8765\n",
      "Epoch 16/30\n",
      "39665/39665 [==============================] - 12s 306us/sample - loss: 0.2647 - accuracy: 0.8914 - val_loss: 0.2835 - val_accuracy: 0.8830\n",
      "Epoch 17/30\n",
      "39665/39665 [==============================] - 12s 292us/sample - loss: 0.2643 - accuracy: 0.8932 - val_loss: 0.2877 - val_accuracy: 0.8773\n",
      "Epoch 18/30\n",
      "39665/39665 [==============================] - 13s 315us/sample - loss: 0.2637 - accuracy: 0.8930 - val_loss: 0.2860 - val_accuracy: 0.8852\n",
      "Epoch 19/30\n",
      "39665/39665 [==============================] - 12s 304us/sample - loss: 0.2607 - accuracy: 0.8949 - val_loss: 0.2778 - val_accuracy: 0.8832\n",
      "Epoch 20/30\n",
      "39665/39665 [==============================] - 13s 318us/sample - loss: 0.2601 - accuracy: 0.8932 - val_loss: 0.2751 - val_accuracy: 0.8875\n",
      "Epoch 21/30\n",
      "39665/39665 [==============================] - 13s 323us/sample - loss: 0.2595 - accuracy: 0.8940 - val_loss: 0.2786 - val_accuracy: 0.8843\n",
      "Epoch 22/30\n",
      "39665/39665 [==============================] - 12s 301us/sample - loss: 0.2560 - accuracy: 0.8948 - val_loss: 0.2750 - val_accuracy: 0.8880\n",
      "Epoch 23/30\n",
      "39665/39665 [==============================] - 12s 299us/sample - loss: 0.2551 - accuracy: 0.8949 - val_loss: 0.2736 - val_accuracy: 0.8867\n",
      "Epoch 24/30\n",
      "39665/39665 [==============================] - 12s 298us/sample - loss: 0.2543 - accuracy: 0.8963 - val_loss: 0.2690 - val_accuracy: 0.8885\n",
      "Epoch 25/30\n",
      "39665/39665 [==============================] - 12s 313us/sample - loss: 0.2541 - accuracy: 0.8960 - val_loss: 0.2960 - val_accuracy: 0.8696\n",
      "Epoch 26/30\n",
      "39665/39665 [==============================] - 12s 310us/sample - loss: 0.2510 - accuracy: 0.8963 - val_loss: 0.2719 - val_accuracy: 0.8843\n",
      "Epoch 27/30\n",
      "39665/39665 [==============================] - 14s 352us/sample - loss: 0.2509 - accuracy: 0.8972 - val_loss: 0.2910 - val_accuracy: 0.8706\n",
      "Epoch 28/30\n",
      "39665/39665 [==============================] - 13s 329us/sample - loss: 0.2483 - accuracy: 0.8982 - val_loss: 0.2702 - val_accuracy: 0.8865\n",
      "Epoch 29/30\n",
      "39665/39665 [==============================] - 11s 283us/sample - loss: 0.2484 - accuracy: 0.8993 - val_loss: 0.2744 - val_accuracy: 0.8864\n",
      "Epoch 30/30\n",
      "39665/39665 [==============================] - 12s 312us/sample - loss: 0.2474 - accuracy: 0.8989 - val_loss: 0.2746 - val_accuracy: 0.8900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a668ecc90>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=30,\n",
    "          validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict_classes(np.expand_dims(X_train[1],axis=0))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"sentiment_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "classifier = load_model(\"sentiment_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/cloudcompt/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "data = getAvgFeatureVecs([data['text'][0]],model,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataVects = []\n",
    "for item in data:\n",
    "    item[np.isnan(item)] = 0\n",
    "    dataVects.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer sequential_30 is incompatible with the layer: expected axis -1 of input shape to have value 300 but received input with shape [None, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a439d903af47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataVects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda3/envs/cloudcompt/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/cloudcompt/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     return self._model_iteration(\n\u001b[1;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/cloudcompt/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    397\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/cloudcompt/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/cloudcompt/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m       \u001b[0mall_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/cloudcompt/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m   2620\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m       \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2622\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2623\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_dict_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/cloudcompt/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m   2707\u001b[0m           \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2709\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2710\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2711\u001b[0m         \u001b[0;31m# This Model or a submodel is dynamic and hasn't overridden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/cloudcompt/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;31m# are casted, not before.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[0;32m--> 812\u001b[0;31m                                               self.name)\n\u001b[0m\u001b[1;32m    813\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/cloudcompt/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;34m' incompatible with the layer: expected axis '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0;34m' of input shape to have value '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                 ' but received input with shape ' + str(shape))\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;31m# Check shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer sequential_30 is incompatible with the layer: expected axis -1 of input shape to have value 300 but received input with shape [None, 1]"
     ]
    }
   ],
   "source": [
    "classifier.predict(dataVects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
